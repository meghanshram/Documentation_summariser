from langchain.prompts import ChatPromptTemplate
from Backend.llm_gateway import initialize_openai_client
from langchain.prompts import ChatPromptTemplate
from langchain.schema.runnable import RunnableSequence

def create_query_chain(llm):
    """
    Creates a RunnableSequence with a custom prompt tailored for a coding chatbot.
    """
    # Define the prompt template
    prompt = ChatPromptTemplate.from_template(
            "You are a coding assistant trained to answer questions based strictly on the provided documentation.\n\n"
            "Context:\n{context}\n\n"
            "Rules:\n"
            "1. Only use the information in the context to answer the question.\n"
            "2. If the answer is not found in the context, respond with: "
            "'The documentation does not provide this information.'\n"
            "3. Provide clear and concise explanations, and include examples if relevant.\n\n"
            "User Query: {question}\n\n"
            "Your Answer:"
    )
    
    # Create a RunnableSequence pipeline
    chain = prompt | llm
    return chain


def query_llm(chain, context, question):
    """
    Sends a query to the LLM using the RunnableSequence chain.
    :param chain: The RunnableSequence created for querying the LLM.
    :param context: The context extracted from the documentation.
    :param question: The user's question about the documentation.
    :return: The response generated by the LLM.
    """
    inputs = {"context": context, "question": question}
    return chain.invoke(inputs)

# if __name__ == "__main__":
#     # Sample context and question
#     sample_context = (
#         "This is an example documentation about Flask. Flask is a lightweight WSGI web application framework. "
#         "It provides tools, libraries, and technologies for building web applications."
#     )
#     sample_question = "What is Flask used for?"

#     try:
#         # Initialize the OpenAI client
#         llm = initialize_openai_client()  # Use the updated initialize function

#         # Create the query chain
#         query_chain = create_query_chain(llm)

#         # Query the LLM
#         response = query_llm(query_chain, sample_context, sample_question)

#         # Print the response
#         print("\nResponse from LLM:")
#         print(response.content)

#     except Exception as e:
#         print(f"Error during LLM processing: {e}")

